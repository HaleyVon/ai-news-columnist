"""
ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ API ì„œë¹„ìŠ¤
ì •ì¹˜ ê´€ë ¨ ìµœì‹  ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•˜ê³  ë°ì´í„°ë¥¼ ì •ì œí•˜ì—¬ ì œê³µ
"""

import httpx
import logging
import json
import os
from typing import List, Dict, Any, Optional
from datetime import datetime, timedelta, timezone
from urllib.parse import quote
import html
import re

from schemas import Source
from core.exceptions import NewsSearchException

logger = logging.getLogger(__name__)


class NaverNewsSearchService:
    """ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ API ì„œë¹„ìŠ¤ í´ë˜ìŠ¤"""
    
    def __init__(self, client_id: str, client_secret: str):
        """
        ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        
        Args:
            client_id: ë„¤ì´ë²„ API í´ë¼ì´ì–¸íŠ¸ ID
            client_secret: ë„¤ì´ë²„ API í´ë¼ì´ì–¸íŠ¸ Secret
        """
        self.client_id = client_id
        self.client_secret = client_secret
        self.base_url = "https://openapi.naver.com/v1/search/news.json"
        
        # API í—¤ë” ì„¤ì •
        self.headers = {
            "X-Naver-Client-Id": self.client_id,
            "X-Naver-Client-Secret": self.client_secret,
            "User-Agent": "ai-news-columnist/1.0"
        }
    
    async def search_recent_news(
        self, 
        topic: str, 
        max_results: int = 20,
        days_back: int = 7,
        sort_by: str = "date",
        search_mode: str = "title"
    ) -> List[Dict[str, Any]]:
        """
        ì£¼ì œì™€ ê´€ë ¨ëœ ìµœì‹  ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰
        
        Args:
            topic: ê²€ìƒ‰í•  ì£¼ì œ í‚¤ì›Œë“œ
            max_results: ìµœëŒ€ ê²°ê³¼ ê°œìˆ˜ (ê¸°ë³¸ 20ê°œ, ìµœëŒ€ 100ê°œ)
            days_back: ê²€ìƒ‰ ê¸°ê°„ (ì¼ ë‹¨ìœ„, ê¸°ë³¸ 7ì¼)
            sort_by: ì •ë ¬ ë°©ì‹ ('date' ë˜ëŠ” 'sim')
            search_mode: ê²€ìƒ‰ ë²”ìœ„ ('title': ì œëª©ë§Œ, 'all': ì œëª©+ë‚´ìš©)
            
        Returns:
            List[Dict[str, Any]]: ê²€ìƒ‰ëœ ë‰´ìŠ¤ ëª©ë¡
        """
        try:
            logger.info(f"ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ ì‹œì‘: '{topic}' (ìµœê·¼ {days_back}ì¼)")
            
            # ê²€ìƒ‰ ì¿¼ë¦¬ ìµœì í™” - ì •ì¹˜ ê´€ë ¨ í‚¤ì›Œë“œ ì¶”ê°€
            optimized_query = self._optimize_political_query(topic)
            
            # API ìš”ì²­ íŒŒë¼ë¯¸í„° ì„¤ì •
            params = {
                "query": optimized_query,
                "display": min(max_results, 100),  # ìµœëŒ€ 100ê°œë¡œ ì œí•œ
                "start": 1,
                "sort": sort_by  # 'date' (ìµœì‹ ìˆœ) ë˜ëŠ” 'sim' (ê´€ë ¨ë„ìˆœ)
            }
            
            # ë„¤ì´ë²„ API í˜¸ì¶œ
            async with httpx.AsyncClient() as client:
                response = await client.get(
                    self.base_url,
                    headers=self.headers,
                    params=params,
                    timeout=30.0
                )
                
                if response.status_code != 200:
                    logger.error(f"ë„¤ì´ë²„ API í˜¸ì¶œ ì‹¤íŒ¨: {response.status_code}")
                    raise NewsSearchException(f"ë‰´ìŠ¤ ê²€ìƒ‰ API ì˜¤ë¥˜: {response.status_code}")
                
                data = response.json()
                
                # ë‰´ìŠ¤ ë°ì´í„° ì •ì œ ë° í•„í„°ë§ (ê²€ìƒ‰ ëª¨ë“œ ì ìš©)
                news_items = self._process_news_items(data.get("items", []), days_back, search_mode, topic)
                
                logger.info(f"ë‰´ìŠ¤ ê²€ìƒ‰ ì™„ë£Œ: {len(news_items)}ê°œ ë‰´ìŠ¤ ë°œê²¬")
                return news_items
                
        except httpx.TimeoutException:
            logger.error("ë„¤ì´ë²„ API í˜¸ì¶œ íƒ€ì„ì•„ì›ƒ")
            raise NewsSearchException("ë‰´ìŠ¤ ê²€ìƒ‰ íƒ€ì„ì•„ì›ƒ")
        except Exception as e:
            logger.error(f"ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            raise NewsSearchException(f"ë‰´ìŠ¤ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
    
    def _optimize_political_query(self, topic: str) -> str:
        """
        ì •ì¹˜ ê´€ë ¨ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ìµœì í™”
        
        Args:
            topic: ì›ë³¸ ì£¼ì œ
            
        Returns:
            str: ìµœì í™”ëœ ê²€ìƒ‰ ì¿¼ë¦¬
        """
        # ì •ì¹˜ ê´€ë ¨ í‚¤ì›Œë“œ ë§¤í•‘
        political_keywords = {
            "ìœ¤ì„ì—´": "ìœ¤ì„ì—´ ëŒ€í†µë ¹",
            "ì´ì¬ëª…": "ì´ì¬ëª… ë¯¼ì£¼ë‹¹",
            "íƒ„í•µ": "íƒ„í•µ ì •ì¹˜",
            "êµ­ì •ê°ì‚¬": "êµ­ì •ê°ì‚¬ ì •ì¹˜",
            "ì„ ê±°": "ì„ ê±° ì •ì¹˜",
            "ì—¬ë‹¹": "ì—¬ë‹¹ êµ­ë¯¼ì˜í˜",
            "ì•¼ë‹¹": "ì•¼ë‹¹ ë¯¼ì£¼ë‹¹"
        }
        
        # ê¸°ë³¸ ì •ì¹˜ í‚¤ì›Œë“œ ì¶”ê°€
        optimized = topic
        for keyword, enhanced in political_keywords.items():
            if keyword in topic:
                optimized = optimized.replace(keyword, enhanced)
        
        # ì •ì¹˜ ê´€ë ¨ í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ ì •ì¹˜ í‚¤ì›Œë“œ ì¶”ê°€
        if not any(keyword in optimized.lower() for keyword in ["ì •ì¹˜", "ëŒ€í†µë ¹", "êµ­íšŒ", "ì˜ì›", "ë‹¹"]):
            optimized += " ì •ì¹˜"
        
        logger.debug(f"ê²€ìƒ‰ ì¿¼ë¦¬ ìµœì í™”: '{topic}' â†’ '{optimized}'")
        return optimized
    
    def _process_news_items(
        self, 
        items: List[Dict], 
        days_back: int, 
        search_mode: str = "title", 
        topic: str = ""
    ) -> List[Dict[str, Any]]:
        """
        ë„¤ì´ë²„ API ì‘ë‹µ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ê³  ì •ì œ
        
        Args:
            items: ë„¤ì´ë²„ API ì‘ë‹µì˜ ë‰´ìŠ¤ ì•„ì´í…œë“¤
            days_back: í•„í„°ë§í•  ê¸°ê°„ (ì¼ ë‹¨ìœ„)
            search_mode: ê²€ìƒ‰ ë²”ìœ„ ("title": ì œëª©ë§Œ, "all": ì œëª©+ë‚´ìš©)
            topic: ê²€ìƒ‰í•  í‚¤ì›Œë“œ (search_modeê°€ "title"ì¼ ë•Œ ì œëª© í•„í„°ë§ìš©)
            
        Returns:
            List[Dict[str, Any]]: ì •ì œëœ ë‰´ìŠ¤ ëª©ë¡
        """
        processed_items = []
        cutoff_date = datetime.now(timezone.utc) - timedelta(days=days_back)
        
        for item in items:
            try:
                # HTML íƒœê·¸ ì œê±° ë° í…ìŠ¤íŠ¸ ì •ì œ
                title = html.unescape(re.sub(r'<[^>]+>', '', item.get("title", "")))
                description = html.unescape(re.sub(r'<[^>]+>', '', item.get("description", "")))
                
                # ë°œí–‰ì¼ íŒŒì‹± (ë„¤ì´ë²„ í˜•ì‹: "Tue, 03 Sep 2024 10:30:00 +0900")
                pub_date_str = item.get("pubDate", "")
                pub_date = self._parse_naver_date(pub_date_str)
                
                # ê¸°ê°„ í•„í„°ë§ - ì„¤ì •ëœ ê¸°ê°„ ë‚´ì˜ ë‰´ìŠ¤ë§Œ í¬í•¨
                if pub_date and pub_date < cutoff_date:
                    continue
                
                # ì •ì¹˜ ê´€ë ¨ì„± ê²€ì¦
                if not self._is_political_news(title, description):
                    continue
                
                # ê²€ìƒ‰ ëª¨ë“œì— ë”°ë¥¸ í‚¤ì›Œë“œ í•„í„°ë§
                if search_mode == "title" and topic:
                    # "title" ëª¨ë“œ: ì œëª©ì—ì„œë§Œ í‚¤ì›Œë“œ ê²€ìƒ‰
                    topic_keywords = topic.lower().split()
                    title_lower = title.lower()
                    
                    # ëª¨ë“  í‚¤ì›Œë“œê°€ ì œëª©ì— í¬í•¨ë˜ì–´ì•¼ í•¨
                    if not all(keyword in title_lower for keyword in topic_keywords):
                        continue
                # "all" ëª¨ë“œ ë˜ëŠ” topicì´ ì—†ëŠ” ê²½ìš°: ê¸°ì¡´ëŒ€ë¡œ ëª¨ë“  ë‰´ìŠ¤ í—ˆìš©
                
                processed_item = {
                    "title": title,
                    "description": description,
                    "link": item.get("originallink", item.get("link", "")),
                    "pubDate": pub_date.isoformat() if pub_date else pub_date_str,
                    "originalLink": item.get("originallink", ""),
                    "naverLink": item.get("link", "")
                }
                
                processed_items.append(processed_item)
                
            except Exception as e:
                logger.warning(f"ë‰´ìŠ¤ ì•„ì´í…œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                continue
        
        # ë°œí–‰ì¼ ìˆœìœ¼ë¡œ ì •ë ¬ (ìµœì‹ ìˆœ)
        processed_items.sort(key=lambda x: x["pubDate"], reverse=True)
        
        logger.debug(f"ë‰´ìŠ¤ ì •ì œ ì™„ë£Œ: {len(items)}ê°œ â†’ {len(processed_items)}ê°œ")
        return processed_items
    
    def _parse_naver_date(self, date_str: str) -> Optional[datetime]:
        """
        ë„¤ì´ë²„ APIì˜ ë‚ ì§œ í˜•ì‹ì„ íŒŒì‹±
        
        Args:
            date_str: ë„¤ì´ë²„ API ë‚ ì§œ ë¬¸ìì—´
            
        Returns:
            Optional[datetime]: íŒŒì‹±ëœ datetime ê°ì²´
        """
        try:
            # ë„¤ì´ë²„ í˜•ì‹: "Tue, 03 Sep 2024 10:30:00 +0900"
            # strptimeìœ¼ë¡œ íŒŒì‹±
            return datetime.strptime(date_str, "%a, %d %b %Y %H:%M:%S %z")
        except Exception as e:
            logger.warning(f"ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨: {date_str} - {str(e)}")
            return None
    
    def _is_political_news(self, title: str, description: str) -> bool:
        """
        ë‰´ìŠ¤ê°€ ì •ì¹˜ ê´€ë ¨ì¸ì§€ íŒë‹¨
        
        Args:
            title: ë‰´ìŠ¤ ì œëª©
            description: ë‰´ìŠ¤ ì„¤ëª…
            
        Returns:
            bool: ì •ì¹˜ ë‰´ìŠ¤ ì—¬ë¶€
        """
        political_keywords = [
            # ì •ë¶€ ê´€ë ¨
            "ëŒ€í†µë ¹", "ì •ë¶€", "ì²­ì™€ëŒ€", "êµ­ë¬´ì´ë¦¬", "ì¥ê´€", "ì •ë¶€", "í–‰ì •ë¶€",
            # êµ­íšŒ ê´€ë ¨  
            "êµ­íšŒ", "ì˜ì›", "êµ­ì •ê°ì‚¬", "êµ­ì •ì¡°ì‚¬", "ë²•ì•ˆ", "ì…ë²•", "ì˜ì •",
            # ì •ë‹¹ ê´€ë ¨
            "ë¯¼ì£¼ë‹¹", "êµ­ë¯¼ì˜í˜", "ì •ì˜ë‹¹", "ì—¬ë‹¹", "ì•¼ë‹¹", "ì •ì¹˜ì¸", "ì •ë‹¹",
            # ì •ì¹˜ ì´ìŠˆ
            "ì„ ê±°", "íˆ¬í‘œ", "ê³µì•½", "ì •ì±…", "ê°œí—Œ", "íƒ„í•µ", "ì‚¬í‡´", "ì„ëª…",
            # ì •ì¹˜ì  ì‚¬ê±´
            "ì •ì¹˜", "ì™¸êµ", "êµ­ì •", "ì •ë¬´", "ë‚´ê°", "ê¶Œë ¥", "ì •ì¹˜ê¶Œ"
        ]
        
        text = (title + " " + description).lower()
        
        # ì •ì¹˜ í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        return any(keyword in text for keyword in political_keywords)
    
    def convert_to_sources(self, news_items: List[Dict[str, Any]]) -> List[Source]:
        """
        ë‰´ìŠ¤ ë°ì´í„°ë¥¼ Source ìŠ¤í‚¤ë§ˆë¡œ ë³€í™˜
        
        Args:
            news_items: ê²€ìƒ‰ëœ ë‰´ìŠ¤ ì•„ì´í…œë“¤
            
        Returns:
            List[Source]: Source ìŠ¤í‚¤ë§ˆë¡œ ë³€í™˜ëœ ëª©ë¡
        """
        sources = []
        
        for item in news_items:
            try:
                source = Source(
                    title=item["title"],
                    uri=item.get("link", "")
                )
                sources.append(source)
                
            except Exception as e:
                logger.warning(f"Source ë³€í™˜ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                continue
        
        logger.info(f"Source ë³€í™˜ ì™„ë£Œ: {len(sources)}ê°œ")
        return sources
    
    def format_news_for_prompt(self, news_items: List[Dict[str, Any]]) -> str:
        """
        ë‰´ìŠ¤ ë°ì´í„°ë¥¼ í”„ë¡¬í”„íŠ¸ì— ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” í˜•ì‹ìœ¼ë¡œ í¬ë§·
        
        Args:
            news_items: ê²€ìƒ‰ëœ ë‰´ìŠ¤ ì•„ì´í…œë“¤
            
        Returns:
            str: í”„ë¡¬í”„íŠ¸ìš©ìœ¼ë¡œ í¬ë§·ëœ ë‰´ìŠ¤ í…ìŠ¤íŠ¸
        """
        if not news_items:
            return "ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        formatted_news = []
        
        for i, item in enumerate(news_items[:10], 1):  # ìµœëŒ€ 10ê°œë§Œ ì‚¬ìš©
            news_text = f"""
[ë‰´ìŠ¤ {i}]
ì œëª©: {item['title']}
ë‚´ìš©: {item['description']}
ë°œí–‰ì¼: {item['pubDate']}
ë§í¬: {item.get('originalLink', item.get('link', ''))}
"""
            formatted_news.append(news_text.strip())
        
        return "\n\n".join(formatted_news)
    
    def save_news_data_to_json(
        self, 
        news_items: List[Dict[str, Any]], 
        topic: str, 
        save_analysis: bool = True
    ) -> str:
        """
        ë‰´ìŠ¤ ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥í•˜ê³  ë¶„ì„ ì •ë³´ë„ í¬í•¨
        
        Args:
            news_items: ì €ì¥í•  ë‰´ìŠ¤ ë°ì´í„°
            topic: ê²€ìƒ‰ ì£¼ì œ
            save_analysis: ë¶„ì„ ì •ë³´ í¬í•¨ ì—¬ë¶€
            
        Returns:
            str: ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
        """
        try:
            # íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„±
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            safe_topic = "".join(c for c in topic if c.isalnum() or c in (' ', '-', '_')).rstrip()
            safe_topic = safe_topic.replace(' ', '_')
            
            # íŒŒì¼ ê²½ë¡œ ìƒì„±
            filename = f"news_{safe_topic}_{timestamp}.json"
            filepath = os.path.join("data", "collected_news", filename)
            
            # ë””ë ‰í† ë¦¬ í™•ì¸ ë° ìƒì„±
            os.makedirs(os.path.dirname(filepath), exist_ok=True)
            
            # ì €ì¥í•  ë°ì´í„° êµ¬ì¡° ìƒì„±
            save_data = {
                "metadata": {
                    "search_topic": topic,
                    "search_timestamp": datetime.now().isoformat(),
                    "total_results": len(news_items),
                    "data_source": "naver_news_api"
                },
                "news_items": news_items
            }
            
            # ë¶„ì„ ì •ë³´ ì¶”ê°€
            if save_analysis:
                save_data["analysis"] = self._analyze_news_quality(news_items)
            
            # JSON íŒŒì¼ë¡œ ì €ì¥
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(save_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ë‰´ìŠ¤ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {filepath}")
            return filepath
            
        except Exception as e:
            logger.error(f"ë‰´ìŠ¤ ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {str(e)}")
            raise NewsSearchException(f"JSON ì €ì¥ ì‹¤íŒ¨: {str(e)}")
    
    def _analyze_news_quality(self, news_items: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        ë‰´ìŠ¤ ë°ì´í„°ì˜ í’ˆì§ˆì„ ë¶„ì„
        
        Args:
            news_items: ë¶„ì„í•  ë‰´ìŠ¤ ë°ì´í„°
            
        Returns:
            Dict[str, Any]: ë¶„ì„ ê²°ê³¼
        """
        if not news_items:
            return {"error": "ë¶„ì„í•  ë‰´ìŠ¤ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤"}
        
        # description ê¸¸ì´ í†µê³„
        desc_lengths = []
        empty_desc_count = 0
        has_original_link_count = 0
        
        for item in news_items:
            desc = item.get('description', '')
            desc_lengths.append(len(desc))
            
            if not desc.strip():
                empty_desc_count += 1
            
            if item.get('originalLink'):
                has_original_link_count += 1
        
        # í†µê³„ ê³„ì‚°
        avg_desc_length = sum(desc_lengths) / len(desc_lengths) if desc_lengths else 0
        min_desc_length = min(desc_lengths) if desc_lengths else 0
        max_desc_length = max(desc_lengths) if desc_lengths else 0
        
        # ìƒ˜í”Œ ë‰´ìŠ¤ ì„ íƒ (ê°€ì¥ ê¸´ descriptionê³¼ ê°€ì¥ ì§§ì€ ê²ƒ)
        longest_desc_item = max(news_items, key=lambda x: len(x.get('description', '')))
        shortest_desc_item = min(news_items, key=lambda x: len(x.get('description', '')))
        
        analysis = {
            "quality_metrics": {
                "total_items": len(news_items),
                "empty_description_count": empty_desc_count,
                "empty_description_ratio": empty_desc_count / len(news_items),
                "has_original_link_count": has_original_link_count,
                "original_link_ratio": has_original_link_count / len(news_items)
            },
            "description_statistics": {
                "average_length": round(avg_desc_length, 2),
                "min_length": min_desc_length,
                "max_length": max_desc_length,
                "length_distribution": {
                    "very_short_0_50": sum(1 for l in desc_lengths if 0 <= l <= 50),
                    "short_51_100": sum(1 for l in desc_lengths if 51 <= l <= 100),
                    "medium_101_200": sum(1 for l in desc_lengths if 101 <= l <= 200),
                    "long_201_plus": sum(1 for l in desc_lengths if l > 200)
                }
            },
            "sample_items": {
                "longest_description": {
                    "title": longest_desc_item.get('title', ''),
                    "description": longest_desc_item.get('description', ''),
                    "length": len(longest_desc_item.get('description', '')),
                    "link": longest_desc_item.get('originalLink', '')
                },
                "shortest_description": {
                    "title": shortest_desc_item.get('title', ''),
                    "description": shortest_desc_item.get('description', ''),
                    "length": len(shortest_desc_item.get('description', '')),
                    "link": shortest_desc_item.get('originalLink', '')
                }
            }
        }
        
        return analysis
    
    def print_analysis_summary(self, analysis: Dict[str, Any]) -> None:
        """
        ë¶„ì„ ê²°ê³¼ë¥¼ ì½˜ì†”ì— ì¶œë ¥
        
        Args:
            analysis: ë¶„ì„ ê²°ê³¼ ë°ì´í„°
        """
        print("\n" + "="*60)
        print("ğŸ“Š ë‰´ìŠ¤ ë°ì´í„° í’ˆì§ˆ ë¶„ì„ ê²°ê³¼")
        print("="*60)
        
        if "error" in analysis:
            print(f"âŒ ë¶„ì„ ì˜¤ë¥˜: {analysis['error']}")
            return
        
        metrics = analysis.get('quality_metrics', {})
        desc_stats = analysis.get('description_statistics', {})
        samples = analysis.get('sample_items', {})
        
        # ê¸°ë³¸ í†µê³„
        print(f"ğŸ“ ì „ì²´ ë‰´ìŠ¤ ê°œìˆ˜: {metrics.get('total_items', 0)}ê°œ")
        print(f"ğŸ”— ì›ë³¸ ë§í¬ ë³´ìœ : {metrics.get('has_original_link_count', 0)}ê°œ ({metrics.get('original_link_ratio', 0)*100:.1f}%)")
        print(f"âŒ ë¹ˆ ì„¤ëª… ë‰´ìŠ¤: {metrics.get('empty_description_count', 0)}ê°œ ({metrics.get('empty_description_ratio', 0)*100:.1f}%)")
        
        # Description ê¸¸ì´ í†µê³„
        print(f"\nğŸ“ Description ê¸¸ì´ í†µê³„:")
        print(f"   í‰ê·  ê¸¸ì´: {desc_stats.get('average_length', 0):.1f}ì")
        print(f"   ìµœì†Œ ê¸¸ì´: {desc_stats.get('min_length', 0)}ì")
        print(f"   ìµœëŒ€ ê¸¸ì´: {desc_stats.get('max_length', 0)}ì")
        
        # ê¸¸ì´ë³„ ë¶„í¬
        dist = desc_stats.get('length_distribution', {})
        print(f"\nğŸ“Š ê¸¸ì´ë³„ ë¶„í¬:")
        print(f"   ë§¤ìš° ì§§ìŒ (0-50ì): {dist.get('very_short_0_50', 0)}ê°œ")
        print(f"   ì§§ìŒ (51-100ì): {dist.get('short_51_100', 0)}ê°œ") 
        print(f"   ë³´í†µ (101-200ì): {dist.get('medium_101_200', 0)}ê°œ")
        print(f"   ê¹€ (201ì+): {dist.get('long_201_plus', 0)}ê°œ")
        
        # ìƒ˜í”Œ ë‰´ìŠ¤
        if 'longest_description' in samples:
            longest = samples['longest_description']
            print(f"\nğŸ† ê°€ì¥ ê¸´ Description ({longest.get('length', 0)}ì):")
            print(f"   ì œëª©: {longest.get('title', '')[:100]}...")
            print(f"   ë‚´ìš©: {longest.get('description', '')[:200]}...")
        
        if 'shortest_description' in samples:
            shortest = samples['shortest_description']
            print(f"\nğŸ ê°€ì¥ ì§§ì€ Description ({shortest.get('length', 0)}ì):")
            print(f"   ì œëª©: {shortest.get('title', '')[:100]}...")
            print(f"   ë‚´ìš©: '{shortest.get('description', '')}'")
        
        print("\n" + "="*60)
    
    async def search_recent_news_enhanced(
        self,
        topic: str,
        max_results: int = 50,
        days_back: int = 7,
        use_keyword_expansion: bool = True
    ) -> List[Dict[str, Any]]:
        """
        í™•ì¥ëœ ë‰´ìŠ¤ ê²€ìƒ‰: í‚¤ì›Œë“œ ë‹¤ì–‘í™” ë° ë‹¤ì¤‘ ê²€ìƒ‰ìœ¼ë¡œ ë” ë§ì€ ë°ì´í„° ìˆ˜ì§‘
        
        Args:
            topic: ê¸°ë³¸ ê²€ìƒ‰ ì£¼ì œ
            max_results: ìµœëŒ€ ê²°ê³¼ ê°œìˆ˜ (ê¸°ë³¸ 50ê°œ)
            days_back: ê²€ìƒ‰ ê¸°ê°„ (ì¼ ë‹¨ìœ„, ê¸°ë³¸ 7ì¼)
            use_keyword_expansion: í‚¤ì›Œë“œ í™•ì¥ ì‚¬ìš© ì—¬ë¶€
            
        Returns:
            List[Dict[str, Any]]: í™•ì¥ëœ ë‰´ìŠ¤ ëª©ë¡ (ì¤‘ë³µ ì œê±°ë¨)
        """
        try:
            logger.info(f"í™•ì¥ ë‰´ìŠ¤ ê²€ìƒ‰ ì‹œì‘: '{topic}' (í‚¤ì›Œë“œ í™•ì¥: {use_keyword_expansion})")
            
            all_news = []
            seen_urls = set()  # ì¤‘ë³µ ì œê±°ìš©
            
            # 1. ê¸°ë³¸ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰ (ìµœì‹ ìˆœ)
            logger.info("1ë‹¨ê³„: ê¸°ë³¸ í‚¤ì›Œë“œ ê²€ìƒ‰ (ìµœì‹ ìˆœ)")
            news_date = await self.search_recent_news(
                topic=topic,
                max_results=min(max_results, 40),
                days_back=days_back,
                sort_by="date"
            )
            
            for news in news_date:
                url = news.get('originalLink') or news.get('link')
                if url and url not in seen_urls:
                    seen_urls.add(url)
                    all_news.append(news)
            
            # 2. ê¸°ë³¸ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰ (ê´€ë ¨ë„ìˆœ)
            logger.info("2ë‹¨ê³„: ê¸°ë³¸ í‚¤ì›Œë“œ ê²€ìƒ‰ (ê´€ë ¨ë„ìˆœ)")
            news_sim = await self.search_recent_news(
                topic=topic,
                max_results=min(max_results, 40),
                days_back=days_back,
                sort_by="sim"
            )
            
            for news in news_sim:
                url = news.get('originalLink') or news.get('link')
                if url and url not in seen_urls:
                    seen_urls.add(url)
                    all_news.append(news)
            
            # 3. í‚¤ì›Œë“œ í™•ì¥ ê²€ìƒ‰
            if use_keyword_expansion:
                expanded_keywords = self._generate_expanded_keywords(topic)
                
                for i, keyword in enumerate(expanded_keywords[:3], 1):  # ìµœëŒ€ 3ê°œê¹Œì§€
                    logger.info(f"{i+2}ë‹¨ê³„: í™•ì¥ í‚¤ì›Œë“œ '{keyword}' ê²€ìƒ‰")
                    
                    try:
                        expanded_news = await self.search_recent_news(
                            topic=keyword,
                            max_results=30,
                            days_back=days_back,
                            sort_by="date"
                        )
                        
                        for news in expanded_news:
                            url = news.get('originalLink') or news.get('link')
                            if url and url not in seen_urls:
                                seen_urls.add(url)
                                all_news.append(news)
                                
                    except Exception as e:
                        logger.warning(f"í™•ì¥ í‚¤ì›Œë“œ '{keyword}' ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
                        continue
            
            # 4. ê²°ê³¼ ì •ë ¬ (ìµœì‹ ìˆœ)
            all_news.sort(key=lambda x: x.get("pubDate", ""), reverse=True)
            
            # 5. ìµœëŒ€ ê²°ê³¼ ìˆ˜ë¡œ ì œí•œ
            final_news = all_news[:max_results]
            
            logger.info(f"í™•ì¥ ê²€ìƒ‰ ì™„ë£Œ: {len(final_news)}ê°œ ë‰´ìŠ¤ (ì¤‘ë³µ ì œê±°ë¨)")
            return final_news
            
        except Exception as e:
            logger.error(f"í™•ì¥ ë‰´ìŠ¤ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            # ì‹¤íŒ¨ì‹œ ê¸°ë³¸ ê²€ìƒ‰ìœ¼ë¡œ í´ë°±
            return await self.search_recent_news(topic, max_results//2, days_back)
    
    def _generate_expanded_keywords(self, topic: str) -> List[str]:
        """
        ê¸°ë³¸ ì£¼ì œë¥¼ ë°”íƒ•ìœ¼ë¡œ í™•ì¥ í‚¤ì›Œë“œ ìƒì„±
        
        Args:
            topic: ê¸°ë³¸ ì£¼ì œ
            
        Returns:
            List[str]: í™•ì¥ëœ í‚¤ì›Œë“œ ëª©ë¡
        """
        expanded_keywords = []
        
        # ì •ì¹˜ì¸/ì •ë‹¹ë³„ í™•ì¥ íŒ¨í„´
        political_expansions = {
            "ì¡°êµ­": ["ì¡°êµ­í˜ì‹ ë‹¹", "ì¡°êµ­ ëŒ€í‘œ", "ì¡°êµ­í˜ì‹ ì •ì±…ì—°êµ¬ì›"],
            "ì¡°êµ­í˜ì‹ ë‹¹": ["ì¡°êµ­", "ê°•ë¯¸ì •", "ì¡°êµ­í˜ì‹ ë‹¹ ì„±ë¹„ìœ„", "ì¡°êµ­í˜ì‹ ë‹¹ ëŒ€ë³€ì¸"],
            "ìœ¤ì„ì—´": ["ìœ¤ì„ì—´ ëŒ€í†µë ¹", "ìœ¤ì„ì—´ ì •ë¶€", "ëŒ€í†µë ¹ì‹¤"],
            "ì´ì¬ëª…": ["ì´ì¬ëª… ëŒ€í‘œ", "ë”ë¶ˆì–´ë¯¼ì£¼ë‹¹", "ë¯¼ì£¼ë‹¹"],
            "íƒ„í•µ": ["íƒ„í•µì†Œì¶”", "íƒ„í•µ ì •ì¹˜", "êµ­ì •ì¡°ì‚¬"],
            "êµ­ì •ê°ì‚¬": ["êµ­ì •ê°ì‚¬ ì •ì¹˜", "êµ­ê°", "êµ­íšŒ ê°ì‚¬"],
            "ê¸°ìíšŒê²¬": ["ë¸Œë¦¬í•‘", "ê¸°ìê°„ë‹´íšŒ", "ë°œí‘œ"]
        }
        
        # ì£¼ì œì— í¬í•¨ëœ í‚¤ì›Œë“œ ê¸°ë°˜ í™•ì¥
        for key, expansions in political_expansions.items():
            if key in topic:
                expanded_keywords.extend(expansions)
        
        # ì¼ë°˜ì ì¸ ì •ì¹˜ í‚¤ì›Œë“œ ì¡°í•©
        base_terms = topic.split()
        if len(base_terms) > 1:
            # ê°œë³„ ë‹¨ì–´ë“¤ë¡œë„ ê²€ìƒ‰
            for term in base_terms:
                if len(term) > 1:  # í•œ ê¸€ì ì œì™¸
                    expanded_keywords.append(term)
        
        # ì¤‘ë³µ ì œê±° ë° ì›ë³¸ ì œì™¸
        expanded_keywords = list(set(expanded_keywords))
        if topic in expanded_keywords:
            expanded_keywords.remove(topic)
        
        logger.debug(f"í‚¤ì›Œë“œ í™•ì¥: '{topic}' â†’ {expanded_keywords}")
        return expanded_keywords